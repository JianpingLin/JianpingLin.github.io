<html lang="en">
    <head>
        <meta http-equiv="content-type" content="text/html; charset=utf-8">
        <title>
            Jianping Lin's Homepage
        </title>
        <meta name="viewport" content="width=device-width, initial-scale=1.0"><!-- Le styles -->
        <link href="assets/css/bootstrap.min.css" rel="stylesheet" type="text/css">
        <link href="assets/css/bootstrap-responsive.min.css" rel="stylesheet" type="text/css">
        <link href="assets/css/yangqing.css" rel="stylesheet" type="text/css"><!-- Le HTML5 shim, for IE6-8 support of HTML5 elements -->
        <script src="assets/js/jquery.js"></script>
        <script src="assets/js/bootstrap.min.js"></script>
    </head>
    <body>
        <div class="container">
            <div class="row">
                <div class="span3 bs-docs-sidebar">
                    <hr class="hidden-phone">
                    <div class="text-center hidden-phone">
                        <img src="assets/imgs/JianpingLin.jpg" alt="photo" class="logo-image">
                    </div>
                </div>
                <div class="span9">
                    <h3>
                        Jianping Lin (林建平)
                    </h3>
                    <h5>
                        <a target="_blank" href="CV_JianpingLin.pdf">CV</a> /
			<a target="_blank" href="https://github.com/JianpingLin">GitHub</a> /
		        <a target="_blank" href="https://scholar.google.com/citations?hl=en&view_op=list_works&gmla=AJsN-F6-zR5Ns4cKXA4OKOayXYV5W_09RLmFkupkIyO-cTK02WoPaTSDoHoNvYonuZLsLPCO3mwbhQyw0tkGWRQFB9DhC5XZYyiMtBql6ZsYKifOx0g-wtw&user=R5ecu5oAAAAJ">Google Scholar</a> /
            <a target="_blank" href="https://orcid.org/0000-0001-6930-3104">ORCID</a> /
			    Email: ljp105@mail.ustc.edu.cn  |  jianping_lin@sfu.ca

                    </h5>
                     <a class="visible-phone pull-left" href="#">
			     <img class="media-object" src="assets/imgs/JianpingLin.jpg" width="200px" style="margin: 0px 10px">
			</a>
    <p>
    I am currently a PhD. candidate at <strong>University of Science and Technology of China (USTC)</strong>, supervised by Prof. <a href="http://staff.ustc.edu.cn/~dongeliu/">Dong Liu</a> and Prof. <a href="http://staff.ustc.edu.cn/~lihq/">Houqiang Li</a>.
    I will graduate in June 2021. I am also a visting research student at <strong>Simon Fraser University (SFU)</strong> in Canada, supervised by Prof. <a href="https://www.sfu.ca/~jiel/">Jie Liang</a>.
    I have worked on deep learning based video coding for more than three years. Currently, I focus on end-to-end learned video compression.
		</p>

    <p>
      I obtained my bachelor degree from University of Science and Technology of China (USTC) in 2016.
		</p>

    <!--
     *** Publications ***
    -->
    <br><br>
    <h3>
        <a name='publications' id="publications"></a> Publications and Manuscripts
    </h3>

    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/M-LVC.PNG" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>M-LVC: Multiple Frames Prediction for Learned Video Compression</strong>
            </h4>
            <strong>Jianping Lin</strong>, Dong Liu, Houqiang Li, Feng Wu<br>

          <i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020</i><br>
          <h5><a data-toggle="collapse" href="#collapse1">[abstract]</a> <a href="https://github.com/JianpingLin/M-LVC_CVPR2020">[code]</a><br></h5>
          <div id="collapse1" class="panel-collapse collapse">
            <div class="panel-body">
                <p>We propose an end-to-end learned video compression scheme for low-latency scenarios. Previous methods are limited in using the previous one frame as reference. Our method introduces the usage of the previous multiple frames as references. In our scheme, the motion vector (MV) field is calculated between the current frame and the previous one. With multiple reference frames and associated multiple MV fields, our designed network can generate more accurate prediction of the current frame, yielding less residual. Multiple reference frames also help generate MV prediction, which reduces the coding cost of MV field. We use two deep auto-encoders to compress the residual and the MV, respectively. To compensate for the compression error of the auto-encoders, we further design a MV refinement network and a residual refinement network, taking use of the multiple reference frames as well. All the modules in our scheme are jointly optimized through a single rate-distortion loss function. We use a step-by-step training strategy to optimize the entire scheme. Experimental results show that the proposed method outperforms the existing learned video compression methods for low-latency mode. Our method also performs better than H.265 in both PSNR and MS-SSIM. Our code and models are publicly available.
                </p>
            </div>
          </div>

        </div>
    </div>

    <br><br>
    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/BARC.PNG" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Convolutional Neural Network-Based Block Up-Sampling for HEVC</strong>
            </h4>
            <strong>Jianping Lin</strong>, Dong Liu, Haitao Yang, Houqiang Li, Feng Wu<br>
          <i>IEEE Transactions on Circuits and Systems for Video Technology</i><br>
          <h5><a data-toggle="collapse" href="#collapse2">[abstract]</a> <a href="https://ieeexplore.ieee.org/abstract/document/8554306">Paper</a><br></h5>
          <div id="collapse2" class="panel-collapse collapse">
            <div class="panel-body">
                <p>Recently, convolutional neural network (CNN)-based methods have achieved remarkable progress in image and video super-resolution, which inspires research on down-/up-sampling-based image and video coding using CNN. Instead of hand-crafted filters for up-sampling, trained CNN models are believed to be more capable of improving image quality, thus leading to coding gain. However, previous studies either concentrated on intra-frame coding or performed down- and up-sampling of entire frame. In this paper, we introduce block-level down- and up-sampling into inter-frame coding with the help of CNN. Specifically, each block in the P or B frame can either be compressed at the original resolution or down-sampled and compressed at low resolution and then, up-sampled by the trained CNN models. Such block-level adaptivity is flexible to cope with the spatially variant texture and motion characteristics. We further investigate how to enhance the capability of CNN-based up-sampling by utilizing reference frames and study how to train the CNN models by using encoded video sequences. We implement the proposed scheme onto the high efficiency video coding (HEVC) reference software and perform a comprehensive set of experiments to evaluate our methods. The experimental results show that our scheme achieves superior performance to the HEVC anchor, especially at low bit rates, leading to an average 3.8%, 2.6%, and 3.5% BD-rate reduction on the HEVC common test sequences under random-access, low-delay B, and low-delay P configurations, respectively. When tested on high-definition and ultrahigh-definition sequences, the average BD-rate exceeds 5%.
                </p>
            </div>
          </div>
        </div>
    </div>

    <br><br>
    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/DLVC.PNG" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Deep Learning-Based Video Coding: A Review and A Case Study</strong>
            </h4>
            Dong Liu, Yue Li, <strong>Jianping Lin</strong>, Houqiang Li, Feng Wu<br>

          <i>ACM Computing Surveys (CSUR)</i>
          <h5><a data-toggle="collapse" href="#collapse3">[abstract]</a> <a href="https://arxiv.org/abs/1904.12462">[arXiv]</a> <a href="https://dl.acm.org/doi/10.1145/3368405">[paper]</a><br></h5>
          <div id="collapse3" class="panel-collapse collapse">
            <div class="panel-body">
                <p>The past decade has witnessed the great success of deep learning in many disciplines, especially in computer vision and image processing. However, deep learning-based video coding remains in its infancy. We review the representative works about using deep learning for image/video coding, an actively developing research area since 2015. We divide the related works into two categories: new coding schemes that are built primarily upon deep networks, and deep network-based coding tools that shall be used within traditional coding schemes. For deep schemes, pixel probability modeling and auto-encoder are the two approaches, that can be viewed as predictive coding and transform coding, respectively. For deep tools, there have been several techniques using deep learning to perform intra-picture prediction, inter-picture prediction, cross-channel prediction, probability distribution prediction, transform, post- or in-loop filtering, down- and up-sampling, as well as encoding optimizations. In the hope of advocating the research of deep learning-based video coding, we present a case study of our developed prototype video codec, Deep Learning Video Coding (DLVC). DLVC features two deep tools that are both based on convolutional neural network (CNN), namely CNN-based in-loop filter and CNN-based block adaptive resolution coding. The source code of DLVC has been released for future research.
                </p>
            </div>
          </div>
        </div>
    </div>

    <br><br>
    <div class="media">
        <a class="pull-left" href="#"><img class="media-object" src="assets/imgs/VC-LAPGAN.PNG" width="150px" height="150px"></a>
        <div class="media-body">
            <p class="media-heading">
                <a href="#"></a>
            </p>
            <h4>
                <strong>Generative Adversarial Network-Based Frame Extrapolation for Video Coding</strong>
            </h4>
            <strong>Jianping Lin</strong>, Dong Liu, Houqiang Li, Feng Wu<br>
          <i>IEEE Visual Communications and Image Processing (VCIP), 2018</i>
          <a href="https://ieeexplore.ieee.org/document/8698615">[paper]</a><br>
        </div>
    </div>

    <!-- *** Software Development *** -->
    <br><br>
    			<h3>
        <a name='Software Development'></a> Software Development
    </h3>

    <div class="media">
    <table>
    <tr>
    <td>The reference software of Deep Learning-Based Video Coding (DLVC) <a href="http://dlvc.bitahub.com/">[bitahub]</a> <a href="https://github.com/FVC2018/DLVC">[github]</a> <a href="https://hub.docker.com/u/huzi96">[dockerhub]</a></td>
    </tr>
    </table>
    </div>

    <!-- *** Professional activities *** -->
    <br><br>
    			<h3>
        <a name='Professional Activities'></a> Professional Activities
    </h3>

    <div class="media">
    <table>
    <tr>
    <td>Reviewer for TMM</td>
    </tr>
    </table>
    </div>

    <!-- *** Honours and Awards *** -->
    <br><br>
    <h3>
        <a name='Honours and Awards'></a> Honours and Awards
    </h3>

    <table>
    <tr>
    <td>Scholarship under the State Scholarship Fund (CSC)</td>
    <td> 2019</td>
    </tr>

    <tr>
    <td>National Encouragement Scholarship</td>
    <td> 2015</td>
    </tr>

    <tr>
    <td>Outstanding Student Scholarship (Grade 2) of USTC</td>
    <td> 2014</td>
    </tr>

    <tr>
    <td>Outstanding Student Scholarship (Grade 2) of Institudte of Modern Physics, China Academy of Sciences(IMPCAS)</td>
    <td> 2013</td>
    </tr>
    </table>

<hr>
<div class="container">
<div class="row">
<div class="span12">
</div>
</div>
</div>

</div>
</div>
</div>
</body>
</html>
